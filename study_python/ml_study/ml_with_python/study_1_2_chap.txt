📌 계열도 설명

Machine Learning (머신러닝)

    선형 모델 (Linear Models)
        선형 회귀, 로지스틱 회귀, Ridge/Lasso

    트리 기반 (Tree-Based)
        결정 트리, 랜덤 포레스트, Extra Trees

    앙상블 기법 (Ensemble Methods)
        Bagging, Boosting, XGBoost/LightGBM

    거리/확률 기반 (Distance/Probabilistic)
        KNN, 나이브 베이즈

    신경망 계열 (Neural Networks → Deep Learning)
        MLP(다층 퍼셉트론), CNN, RNN/LSTM, Transformer


📌 make_forge()
소속: mglearn.datasets.make_forge()
목적: 작은 2D 분류 데이터셋을 생성해서 시각화와 기본 알고리즘 비교 실습에 쓰임.

출력: (X, y)
X: shape (26, 2) — 26개의 샘플, 각 샘플은 2개의 특성(feature)을 가짐
y: shape (26,) — 0 또는 1의 이진 분류 라벨



📌 make_wave()
소속: mglearn.datasets.make_wave()
목적: 1차원 회귀 데이터셋을 생성해서 회귀 모델 실습에 사용.

출력: (X, y)
X: shape (40, 1) — 40개의 샘플, 각 샘플은 하나의 특성
y: shape (40,) — 실수값 타겟



모델                목적            정규화      특성 선택               출력 범위       주요 사용
Linear Regression   연속값 예측     없음        없음                    실수 전체       회귀
Ridge Regression    연속값 예측     L2          없음                    실수 전체       회귀, 다중공선성 완화
Lasso Regression    연속값 예측     L1          있음(계수 0)            실수 전체       회귀, 변수 선택
Logistic Regression 이진/다중 분류  L1/L2 가능  L1 사용시 가능          0~1 확률        분류
SVM                 분류/회귀       C로 규제    없음(특성 선택 아님)    클래스 레이블   분류/회귀




1 규제 (Lasso, L1 regularization)

정의: 가중치 절댓값의 합을 패널티로 더하는 방식    ∑(y - wx)^2 + α∑(w^2)

효과: 일부 가중치를 정확히 0으로 만들어서 변수 선택 효과를 냄 (희소 모델 생성)

특징:
해석이 쉬움 (중요한 변수만 남김)
변수 수가 많고 그중 일부만 중요할 때 유리
상관관계 높은 변수 중 일부만 선택하는 경향


L2 규제 (Ridge, L2 regularization)

정의: 가중치 제곱합을 패널티로 더하는 방식    ∑(y - wx)^2 + α|w|

효과: 모든 가중치를 조금씩 줄임 → 0에 가깝게 만들지만 정확히 0은 아님

특징:
다중공선성 문제 완화
안정적인 모델 생성
모든 변수를 일정 부분 활용하고 싶을 때 유리


다중 분류 전략
1) One-vs-Rest (OvR, 일대다)

    각 클래스를 그 외 전부와 구분하는 이진 분류기를 K개 학습.
    예: 클래스 {A,B,C} → A vs not-A, B vs not-B, C vs not-C (총 3개 모델)
    장점: 단순·빠름, 불균형에도 비교적 안정.
    scikit-learn: LogisticRegression(multi_class='ovr'), LinearSVC(기본이 OvR).

2) One-vs-One (OvO, 일대일)

    클래스 쌍마다 이진 분류기를 학습(총 K·(K-1)/2개).
    예: A vs B, A vs C, B vs C.
    장점: 경계가 복잡할 때 유리, 각 모델이 적은 데이터만 봐 속도 이점 있을 때도.
    scikit-learn: SVC는 기본이 OvO.

3) 다항 로지스틱(Softmax, Multinomial)

    한 번에 K클래스를 공동으로 학습(확률의 총합 1).
    선형 결정경계지만, 특성 변환과 함께 쓰면 강력.
    scikit-learn: LogisticRegression(multi_class='multinomial', solver='lbfgs' 또는 'saga').
    multi_class='auto'면 데이터/solver에 따라 자동으로 multinomial 또는 OvR 선택


SGD(Stochastic Gradient Descent)): 확률적 경사하강법

    전체 데이터를 한 번에 사용하는 배치 경사하강법과 달리, 하나의 샘플(또는 작은 미니배치를 무작위로 뽑아 파라미터를 업데이트
    학습 속도가 빨라지고, 대규모 데이터나 온라인 학습(실시간 데이터)에 적합
    sklearn.linear_model.SGDClassifier / SGDRegression


나이브 베이즈 분류기(Naive Bayes Classifier)
    GaussianNB: 연속 데이터에 사용  (매우 고차원 데이터에 사용)
    BernoulliNB: 이진 데이터에 사용      ┐ alpha 매개변수 가짐
    MultinomialNB: 카운트 데이터에 사용  ┘ (텍스트 분류 같은 희소한 뎅터 카운트 사용)
            희소 데이터 == 대부분의 값이 0인 데이터


1️⃣ 기본 개념 비교
항목        선형 모델                                       나이브 베이즈
아이디어    특성들의 가중합이 결정 경계를 만든다고 가정     베이즈 정리 기반, 조건부 독립 가정
학습 방식   손실 함수 최소화(최적화 기반)                   확률 추정(빈도나 분포 파라미터 추정)
결정 경계   선형(직선, 평면, 초평면)                        로그-우도 공간에서 선형이지만, 데이터 표현에 따라 다름


2️⃣ 데이터 전처리 & 특성 처리
선형 모델
    스케일링(StandardScaler, MinMaxScaler) 영향 큼
    특성 간 상관관계도 모델이 활용 가능
    규제(L1, L2)로 특성 선택·과적합 방지 가능

나이브 베이즈
    스케일링 필요 없음 (확률 계산이므로)
    특성 간 상관관계를 무시하는 조건부 독립 가정
    희소 데이터(BoW, TF-IDF)에서 강점


4️⃣ 예측 확률 해석
선형 모델
    로지스틱 회귀 사용 시 확률 값이 비교적 교정(calibration) 잘 됨
    결정 경계 근처에서는 예측 확률이 민감하게 변함

나이브 베이즈
    확률 값이 극단적으로 나오는 경우 많음(과신, overconfident)
    순위(클래스 비교)는 잘 맞지만 절대 확률 해석은 조심해야 함


5️⃣ 사용 추천 상황

선형 모델 추천
    특성 간 상관관계가 중요할 때
    충분한 데이터와 계산 자원
    해석 가능한 가중치(W)를 원할 때

나이브 베이즈 추천
    텍스트 분류, 스팸 필터 등 희소 고차원 데이터
    빠른 베이스라인 모델 필요할 때
    작은 데이터셋에서 빠른 학습·예측 필요할 때


결정트리
    결정 트리는 데이터를 조건문으로 분할해 나가면서 예측하는 모델
    각 노드에서는 하나의 특성과 임계값을 기준으로 데이터를 나누고, 리프 노드에서는 최종적으로 클래스(분류)나 평균값(회귀)을 예측
📌 핵심 정리
    분류: 지니 불순도나 엔트로피를 최소화하는 방향으로 분할
    회귀: 분산(MSE)을 줄이는 방향으로 분할
    장점: 직관적이고 해석이 쉽다
    단점: 과적합 위험이 크다 → max_depth, min_samples_split 같은 규제 필요


랜덤 포레스트(Random Forest)/그래디언트 부스팅(Gradient Boosting)

공통점
    -여러 개의 얕은 결정 트리를 사용해서 하나의 강력한 모델을 만듭니다.
    -단일 트리보다 일반화 성능이 좋고, 과적합 위험이 줄어듭니다.

차이점

1. 학습 방식
랜덤 포레스트
    -트리들을 독립적으로 학습합니다.
    -각 트리는 부트스트랩 샘플(행 랜덤 추출) + max_features(열 랜덤 추출)로 만들어집니다.
    -최종 예측은 **평균(회귀)**이나 **투표(분류)**로 집계합니다.
    -병렬 학습이 가능 → 속도가 빠르고 안정적.

그래디언트 부스팅
    -트리들을 순차적으로 학습합니다.
    -새로운 트리는 항상 이전 트리들의 **오차(잔차)**를 줄이도록 학습합니다.
    -최종 예측은 모든 트리의 가중 합.
    -순차 학습이라 병렬화가 어렵지만, 성능은 더 강력.

2. 무작위성
    랜덤 포레스트 → 무작위성이 강함 (샘플, 특성 랜덤 선택). → 안정적이고 과적합 잘 방지.
    그래디언트 부스팅 → 무작위성은 약하고, 주로 “오차 보정”에 초점. → 잘 튜닝하면 더 높은 성능.

3. 과적합 경향
    랜덤 포레스트: 트리 수를 늘리면 안정적으로 성능이 좋아짐.
    그래디언트 부스팅: 트리를 너무 많이 쌓으면 과적합되기 쉬움 → **학습률(learning rate)**과 트리 수를 적절히 조절


신경망
    1. 은닉 유닛 (Hidden Unit)
        정의: 은닉층 내부의 개별 뉴런(노드) 하나를 가리킴.

    2. 은닉 층 (Hidden Layer)
        정의: 입력층(Input Layer)과 출력층(Output Layer) 사이에 있는 "전체 층"을 말함.

    MLPClassifier(hidden_layer_sizes= [10, 10])의 의미 (scikit-learn MLP 기준)
        리스트 길이 = 은닉층 개수,
        리스트 안의 값 = 해당 층의 유닛 개수
        ps) alpha=복잡도 제어
            solver= 'adam', 'lbfgs', 'sgd'

분류 예측의 불확실성 추정(uncertainty estimation in classification)
    모델이 내린 예측 결과가 얼마나 신뢰할 수 있는지를 정량적으로 평가하는 방법
    모델이 답에 얼마나 확신하는지 봄