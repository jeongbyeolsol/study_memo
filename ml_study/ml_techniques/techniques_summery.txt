| 모델                      | 설명      | 학습 방식     | 주요 특징                 |
| ------------------------- | --------- | ------------- | ------------------------- |
|  로지스틱 회귀            | 분류      | 수식기반 추정 | 확률적, 선형 결정경계     |
|  SVM (서포트 벡터 머신)   | 분류/회귀 | 최적화        | 결정 경계 최대화          |
|  결정 트리                | 분류/회귀 | 규칙 분할     | 해석 쉬움, 시각화 가능    |
|  랜덤 포레스트            | 분류/회귀 | 앙상블        | 여러 트리의 평균          |
|  KNN                      | 분류/회귀 | 저장 기반     | 거리 기반 비교            |
|  나이브 베이즈            | 분류      | 확률 기반     | 단순하지만 빠름           |
|  K-평균(K-means)          | 군집화    | 반복 분류     | 라벨 없음, 비지도 학습    |
|  PCA  (주성분 분석)       | 차원 축소 | 선형 변환     | 시각화, 노이즈 제거       |
|  신경망(MLP, 딥러닝)      | 분류/회귀 | 다층 퍼셉트론 | 비선형 학습, 고성능       |
|  그레이디언트 부스팅      | 분류/회귀 | 앙상블        | 예측력 매우 강력          |



# 지도학습 대표 알고리즘 요약 (A4 2장 요약본)

## 1. 로지스틱 회귀 (Logistic Regression)
- 분류용 선형 모델로, 시그모이드 함수를 이용해 출력 확률을 계산.
- 가설 함수: hθ(x) = 1 / (1 + exp(-θ^T x))
- 손실 함수: 교차 엔트로피 손실
  J(θ) = -(1/m) Σ[y log(hθ(x)) + (1 - y) log(1 - hθ(x))]
- 확률이 0.5 이상이면 클래스 1, 미만이면 클래스 0으로 분류.

## 2. 결정 트리 (Decision Tree)
- 데이터를 조건에 따라 분할해 트리 형태로 분류.
- 불순도 측정: 엔트로피 또는 지니 지수
  Entropy(S) = -Σ pk log₂ pk
- 정보 이득: IG = Entropy(부모) - 가중합(자식 엔트로피)
- 과적합을 방지하기 위해 가지치기(pruning) 필요.

## 3. 서포트 벡터 머신 (SVM)
- 클래스 간의 마진을 최대화하는 초평면을 찾음.
- 결정 경계: w^T x + b = 0
- 목적 함수: min (1/2) ||w||², 제약: yᵢ(w^T xᵢ + b) ≥ 1
- 커널 기법 사용 시 비선형 분류 가능 (예: RBF 커널)

## 4. K-최근접 이웃 (KNN)
- 학습 없이, 예측 시점에 가장 가까운 K개의 이웃을 기준으로 분류.
- 거리 측정: 유클리드 거리 등
  d(x, x') = sqrt(Σ (xᵢ - xᵢ')²)
- K는 홀수로 설정, K가 작을수록 민감하고 클수록 일반화됨.

## 5. 나이브 베이즈 (Naïve Bayes)
- 베이즈 정리에 기반한 확률 분류기, 특징 간 독립성 가정
  P(C|X) = (P(X|C) * P(C)) / P(X)
- 각 특징 xᵢ는 조건부 독립: P(X|C) = Π P(xᵢ|C)
- 다양한 변형: Gaussian, Bernoulli, Multinomial 등

## 6. 인공신경망 (Artificial Neural Network, ANN)
- 노드(뉴런)들이 계층적으로 연결된 구조, 비선형 문제에 강함.
- 뉴런 계산: z = Σ wᵢ xᵢ + b, a = f(z)
- 활성 함수: sigmoid, tanh, ReLU 등
- 학습: 순전파 → 손실 계산 → 역전파로 가중치 업데이트
  w := w - α * ∂J/∂w
- 손실 함수: MSE(회귀), Cross Entropy(분류)

## 마무리 요약
- 로지스틱 회귀: 확률 분류
- 결정 트리: 조건 기반 분기
- SVM: 최대 마진 분류
- KNN: 거리 기반 투표
- 나이브 베이즈: 확률+독립 가정
- 신경망: 비선형, 역전파 기반 학습


https://chatgpt.com/share/6880d1f9-aba8-8012-a237-8d116c0870e8
