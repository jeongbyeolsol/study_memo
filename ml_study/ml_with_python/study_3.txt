정규화

1. StandardScaler (표준화)

방식: 평균 0, 분산 1로 변환 (z-score 표준화)
특징:
    데이터가 정규분포에 가까울 때 효과적
    이상치(outlier)에 민감 (평균, 표준편차가 영향을 많이 받음)
사용 예시:
    선형 회귀, 로지스틱 회귀, SVM, PCA 등 분포 가정이 있는 알고리즘에서 자주 사용



2. RobustScaler (강건 표준화)

방식: 중앙값(median)과 IQR(interquartile range, 사분위 범위)을 이용
특징:
    평균/표준편차 대신 중앙값과 사분위 범위를 쓰므로 이상치에 강함
    데이터 스케일을 맞추되, outlier를 크게 신경 안 쓰고 싶을 때 유리
사용 예시:
    이상치가 많은 금융 데이터, 센서 데이터



3. MinMaxScaler (최소–최대 정규화)

방식: 데이터의 최솟값 최댓값을 [0, 1] 범위로 선형 변환

특징:
    결과가 항상 [0, 1] 범위
    이상치에 매우 민감 (outlier 하나가 있으면 전체 스케일이 왜곡됨)
사용 예시:
    신경망(딥러닝)에서 자주 사용 (입력이 [0,1] 범위일 때 학습 안정적)
    거리 기반 알고리즘(KNN, K-means)



4. Normalizer (정규화)

방식: 샘플 벡터(한 행)를 **단위 벡터(길이=1)**로 변환
특징:
    각 데이터 포인트의 크기보다는 방향에 집중
    “각 샘플의 특성 크기 비율”만 중요할 때 유리
사용 예시:
    텍스트 데이터(TF-IDF 벡터), 코사인 유사도 기반 모델



📌 QuantileTransformer란?

각 특성(feature)의 값들을 **분위수 함수(누적 분포 함수, CDF)**를 이용해 변환
즉, 원래 데이터 분포를 균일 분포(Uniform) 또는 정규 분포(Gaussian)로 매핑
⚙️ 동작 방식
    데이터의 누적 분포 함수(CDF)를 근사
    각 데이터 포인트가 몇 분위수에 속하는지 계산
    선택한 목표 분포에 맞게 다시 매핑
        output_distribution="uniform" (기본값): [0,1] 범위의 균일 분포로 변환
        output_distribution="normal": 평균 0, 분산 1의 표준 정규분포로 변환
📊 특징
    장점
        이상치(outlier) 영향이 크게 줄어듦 (분위수 기반이라 극단값이 눌림)
        분포를 원하는 형태(균일/정규)로 강제할 수 있음
        비정규분포 데이터를 정규분포로 바꿔주는 데 유용 → 선형 모델이나 SVM 성능 개선 가능
    단점
        데이터의 순위를 기준으로 매핑하기 때문에 정보 손실 발생 가능
        학습/테스트 데이터 분포 차이가 클 때는 왜곡 심해짐


📊 요약 표
스케일러        방식            장점                단점                    자주 쓰이는 곳
StandardScaler  평균=0, 분산=1	분포 정규화         outlier 민감            선형모델, SVM, PCA
RobustScaler    중앙값, IQR    	outlier에 강함      데이터 분포 왜곡 가능   이상치 많은 데이터
MinMaxScaler    [0,1] 범위  	직관적, NN 안정화   outlier 민감            딥러닝, KNN, K-means
Normalizer      벡터 길이=1     방향에 집중         크기 정보 손실          텍스트, 코사인 유사도

📌 코사인 유사도(Cosine Similarity): cos각도를 이용해 두 데이터가 얼마나 비슷한지 측정, 크기보다는 방향에 집중


✅ 정리

StandardScaler: 평균=0, 분산=1
RobustScaler: 중앙값/IQR
MinMaxScaler: [0,1] 범위
Normalizer: 벡터 길이=1
QuantileTransformer: 분포 자체를 균일/정규로 변형



1. PCA (Principal Component Analysis, 주성분 분석)

아이디어: 데이터의 분산이 가장 큰 방향(주성분)을 찾아 좌표축을 새로 잡고, 그 축을 따라 데이터를 표현.
방식:
    공분산 행렬 계산
    고유값 분해(Eigendecomposition) → 가장 큰 고유값 방향 = 주성분
    상위 k개의 주성분으로 투영
특징:
    선형(linear) 변환 기법
    데이터의 분산을 최대한 보존하면서 차원 축소
    노이즈 제거 및 압축 효과
한계: 데이터가 선형적으로 잘 설명되지 않으면 정보 손실이 큼

📌 PCA Whitening이란?
    PCA는 원래 데이터를 분산이 큰 순서대로 정렬된 직교 축으로 투영하는 것.
    Whitening은 여기에 각 주성분을 분산 1로 정규화까지 해주는 옵션 -> 상관관계가 없고, 모든 차원의 분산이 동일(=1)인 새로운 데이터셋을 만드는 것



2. NMF (Non-negative Matrix Factorization, 비음수 행렬 분해)

아이디어: 모든 값이 0 이상(비음수)인 행렬을 두 개의 비음수 행렬 곱으로 근사
특징:
    해석 가능성이 높음 → 각 행렬 성분을 “부분(part)”로 해석 가능
    이미지 분해: 얼굴 이미지를 눈, 코, 입 같은 부분으로 분해 가능
    텍스트 마이닝: 문서-단어 행렬을 “주제(topic)”로 분해 가능
한계:
    음수가 포함된 데이터에는 적용 불가
    로컬 최적해(local optimum)에 빠질 수 있음


3. t-SNE (t-Distributed Stochastic Neighbor Embedding)

아이디어: 고차원 공간에서 가까운 점들은 저차원에서도 가깝게, 먼 점들은 멀게 유지 → 비선형 차원 축소
방식:
    고차원 데이터 간 거리를 조건부 확률로 변환
    저차원에서도 비슷한 확률 분포를 만들도록 점들을 배치
    Kullback–Leibler divergence 최소화
특징:
    데이터의 **지역 구조(local structure)**를 잘 보존 → 군집 시각화에 탁월
    2D, 3D 시각화에서 자주 쓰임 (예: 이미지 임베딩, 단어 벡터 시각화)
한계:
    계산 비용 큼 (대규모 데이터셋에 느림)
    결과 해석이 직관적이지 않음 (축 의미가 없음)
    하이퍼파라미터(특히 perplexity)에 민감


📊 요약
기법   	방식                장점                        단점                            대표 활용
PCA     선형 투영           빠름, 분산 최대 보존        비선형 구조 표현 어려움         노이즈 제거, 압축
NMF     비음수 행렬 분해    해석 가능, 부분 기반 표현   음수 데이터 불가, 로컬 최적해   토픽 모델링, 이미지 분해
t-SNE   비선형 임베딩       군집 구조 시각화 탁월       느림, 해석 어려움               고차원 데이터 시각화



K-평균 군집 개념

목표: 주어진 데이터를 **k개의 그룹(클러스터)**으로 나누는 것.
비지도 학습이므로 정답(레이블)이 없어도 사용
각 그룹은 중심점(centroid)을 기준으로 데이터가 가까운 것끼리 묶임

🔹 알고리즘 동작 원리

1. k개의 중심점 초기화
    랜덤으로 k개의 점(centroid)을 선택
2. 할당 단계 (Assignment step)
    각 데이터 포인트를 가장 가까운 중심점(유클리드 거리 기준)에 할당
3. 업데이트 단계 (Update step)
    각 클러스터에 속한 데이터들의 평균값으로 중심점을 갱신
4. 반복
    더 이상 중심점이 크게 변하지 않을 때까지 2~3단계를 반복

✅ 장점
    구현이 간단하고 계산이 빠름
    대규모 데이터에도 자주 사용

⚠️ 단점
    k값(클러스터 수)을 미리 정해야 함
    원형(구형)에 가까운 클러스터에만 잘 동작 (복잡한 모양에는 약함)
    이상치(outlier)에 민감



병합 군집(Agglomerative Clustering)

🔹 개념
    처음에는 각 데이터를 하나의 군집으로 취급 (n개 데이터 → n개 군집).
    가장 가까운 두 군집을 합치고 → 다시 합치고… 반복.
    마지막에는 하나의 거대한 군집이 될 때까지 병합.

🔹 군집 간 거리 계산 방식 (링크 방법)
    1. 단일 연결(Single Linkage): 두 군집에서 가장 가까운 점들 간의 거리
    2. 완전 연결(Complete Linkage): 두 군집에서 가장 먼 점들 간의 거리
    3. 균 연결(Average Linkage): 두 군집의 모든 점들 간 거리의 평균
    4. 와드 방법(Ward’s Method): 두 군집을 합쳤을 때의 분산 증가량을 최소화

🔹 결과 표현: 덴드로그램(Dendrogram)


DBSCAN (Density-Based Spatial Clustering of Applications with Noise)

🔹 핵심 아이디어
    데이터가 모여 있는 밀도가 높은 영역은 하나의 군집으로 보고, 밀도가 낮은 부분은 노이즈(이상치)로 취급
    밀도 기반 군집화 알고리즘
    "가까이 많이 몰려 있는 점" → 클러스터, "주변에 점이 거의 없는 점" → 잡음(noise)

🔹 주요 매개변수
ε (epsilon)
    이웃을 찾는 반경 (원 크기)
    한 점 주변에서 반경 ε 안에 있는 점들을 “이웃”으로 본다.
minPts (최소 포인트 수)
    하나의 군집으로 인정하려면 그 안에 최소 몇 개의 점이 있어야 하는가?

🔹 데이터 포인트 유형
1.핵심 점(core point)
    반경 ε 안에 최소 minPts 이상의 점이 있으면 핵심 점.
2. 경계 점(border point)
    반경 ε 안에 점은 있지만, minPts보다 적어서 핵심은 아님.
    대신 핵심 점 근처에 속하기 때문에 군집에 포함됨.
3. 노이즈 점(noise point)
    어떤 군집에도 속하지 못한 점.

🔹 장점
    ✅ 군집의 개수를 미리 지정할 필요 없음
    ✅ 임의의 모양(원형, 길쭉한 모양 등)도 군집화 가능
    ✅ 이상치(outlier)를 자동으로 탐지

🔹 단점
    ⚠️ ε과 minPts 값에 민감 (잘못 고르면 성능이 크게 떨어짐)
    ⚠️ 데이터의 밀도가 균일하지 않으면 군집을 잘 못 나눔


1. ARI (Adjusted Rand Index, 보정된 랜드 지수)
    두 군집화 결과(예: 정답 라벨 vs 예측된 클러스터)가 쌍(pair) 단위에서 일치하는 정도를 측정.

값의 범위: -1 ~ 1
    1 → 완벽히 동일한 클러스터링
    0 → 무작위 클러스터링과 비슷
    < 0 → 무작위보다 못한 경우

📌 특징:
    군집 개수가 달라도 비교 가능
    무작위 군집화를 했을 때 기대되는 값이 0이 되도록 보정(adjusted)되어 있음

🔹 2. NMI (Normalized Mutual Information, 정규화 상호정보량)
    정보 이론(information theory)에 기반한 지표
    두 군집화 결과가 공유하는 정보량(=상호정보량)을 계산 → 이를 정규화해서 비교

값의 범위: 0 ~ 1
    1 → 완전히 일치
    0 → 전혀 상관없음

📌 특징:
    라벨 개수(클래스 수)가 달라도 비교 가능
    정보량을 기준으로 하므로 불균형 데이터에도 비교적 강건함

🔹 간단 비교
지표    값 범위 해석	        장점
ARI     -1 ~ 1  1일수록 동일    쌍(pair) 단위 평가, 직관적
NMI     0 ~ 1   1일수록 동일    정보 이론 기반, 불균형에도 강건



실루엣 계수
🔹 핵심 아이디어
    실루엣 계수는 한 데이터가 자기 클러스터에 잘 속했는지를 수치화
    내가 속한 클러스터 내부와는 얼마나 가까운지, 다른 클러스터와는 얼마나 먼지”를 동시에 고려

    a(i): 자기 클러스터 내 다른 점들과의 평균 거리
    b(i): 다른 클러스터들 중 가장 가까운 클러스터까지의 평균 거리
         b(i) - a(i)
s(i) = ---------------
        max(b(i), a(i)

🔹 값의 범위
    +1 에 가까움 → 클러스터에 잘 속함 (자기 클러스터와 가깝고, 다른 클러스터와 멂)
    0 근처 → 경계에 걸쳐 있음
    -1 에 가까움 → 잘못된 클러스터에 속했을 가능성이 큼

🔹 평균 실루엣 계수
    모든 데이터의 실루엣 값을 평균한 값
    클러스터링 전체의 품질을 평가하는 데 사용

🔹 장점
✅ 군집의 개수를 바꿔가며 적절한 k값 선택에 도움
✅ 라벨이 없는 데이터(비지도 학습)에서도 평가 가능